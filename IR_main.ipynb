{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666ec7dd",
   "metadata": {},
   "source": [
    "# Information Retrieval Lab – Vector Space Model (lnc.ltc)\n",
    "\n",
    "This notebook implements a complete Vector Space Model pipeline, including:\n",
    "- Corpus loading from a provided ZIP file\n",
    "- Tokenization, normalization (lowercasing, punctuation removal), stemming (NLTK Porter), and optional lemmatization\n",
    "- Inverted index with dictionary and postings (df, [(docID, tf), ...])\n",
    "- Document vector weighting lnc (log tf + cosine norm, no idf)\n",
    "- Query vector weighting ltc (log tf + idf + cosine norm)\n",
    "- Cosine similarity and ranked retrieval (top-10)\n",
    "- Soundex-based spelling fallback for out-of-vocabulary query terms\n",
    "- Test queries execution with readable outputs\n",
    "\n",
    "Follow the cells in order; each section includes explanations and progress prints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e598f",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26aff114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\arnav jalan\\.conda\\envs\\our\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arnav jalan\\.conda\\envs\\our\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arnav jalan\\.conda\\envs\\our\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\arnav jalan\\.conda\\envs\\our\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 435.7 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.2/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "   -------------------------------------- - 102.4/107.3 kB ? eta -:--:--\n",
      "   -------------------------------------- - 102.4/107.3 kB ? eta -:--:--\n",
      "   -------------------------------------- - 102.4/107.3 kB ? eta -:--:--\n",
      "   -------------------------------------- 107.3/107.3 kB 618.2 kB/s eta 0:00:00\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.3.0 nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb51b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete. NLTK ready.\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# NLTK for stemming/lemmatization\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Ensure needed NLTK resources (quietly)\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Globals populated later\n",
    "DOCS: Dict[int, str] = {}              # docID -> raw text\n",
    "DOCID_TO_PATH: Dict[int, str] = {}     # docID -> relative file path/name\n",
    "PATH_TO_DOCID: Dict[str, int] = {}     # path -> docID\n",
    "VOCAB_DF: Dict[str, int] = {}          # term -> document frequency\n",
    "POSTINGS: Dict[str, List[Tuple[int, int]]] = {}  # term -> list of (docID, raw_tf)\n",
    "DOC_LN_FACTORS: Dict[int, float] = {}  # docID -> l2 norm for lnc\n",
    "N_DOCS: int = 0\n",
    "\n",
    "print(\"Imports complete. NLTK ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ff798",
   "metadata": {},
   "source": [
    "## 2) Unzip corpus and load documents\n",
    "\n",
    "- Unzips the provided `Corpus-*.zip` into a `corpus/` folder next to this notebook.\n",
    "- Loads all `.txt` files into memory.\n",
    "- Assigns incremental docIDs (starting at 0) and keeps mappings both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58688e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping: e:\\COLLEGE MATERIAL\\SEMESTER 7\\Information Retrieval\\Assignment_Vector_Space_Model\\Corpus-20230203T210935Z-001.zip -> e:\\COLLEGE MATERIAL\\SEMESTER 7\\Information Retrieval\\Assignment_Vector_Space_Model\\corpus\n",
      "Loaded 41 documents.\n",
      "Example mapping:\n",
      "  docID=0 -> Corpus\\Adobe.txt\n",
      "  docID=1 -> Corpus\\Amazon.txt\n",
      "  docID=2 -> Corpus\\apple.txt\n"
     ]
    }
   ],
   "source": [
    "# Locate the ZIP file (assumes a single corpus zip in the workspace root)\n",
    "workspace_dir = Path.cwd()\n",
    "zip_candidates = list(workspace_dir.glob('Corpus-*.zip'))\n",
    "if not zip_candidates:\n",
    "    # Also try relative path if launched from subfolder\n",
    "    zip_candidates = list(Path('.').glob('Corpus-*.zip'))\n",
    "\n",
    "if not zip_candidates:\n",
    "    raise FileNotFoundError(\"Corpus ZIP not found. Ensure a file named 'Corpus-*.zip' exists next to this notebook.\")\n",
    "\n",
    "corpus_zip = zip_candidates[0]\n",
    "corpus_dir = workspace_dir / 'corpus'\n",
    "corpus_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Unzipping: {corpus_zip} -> {corpus_dir}\")\n",
    "with zipfile.ZipFile(corpus_zip, 'r') as zf:\n",
    "    zf.extractall(corpus_dir)\n",
    "\n",
    "# Recursively collect .txt files (some zips create a top-level folder)\n",
    "text_files = sorted([p for p in corpus_dir.rglob('*.txt')])\n",
    "if not text_files:\n",
    "    raise RuntimeError(\"No .txt files found after unzipping. Check the archive structure.\")\n",
    "\n",
    "# Load into memory and create ID mappings\n",
    "DOCS.clear(); DOCID_TO_PATH.clear(); PATH_TO_DOCID.clear()\n",
    "doc_id = 0\n",
    "for path in text_files:\n",
    "    try:\n",
    "        text = path.read_text(encoding='utf-8', errors='ignore')\n",
    "    except Exception:\n",
    "        text = path.read_text(errors='ignore')  # fallback\n",
    "    DOCS[doc_id] = text\n",
    "    rel = str(path.relative_to(corpus_dir))\n",
    "    DOCID_TO_PATH[doc_id] = rel\n",
    "    PATH_TO_DOCID[rel] = doc_id\n",
    "    doc_id += 1\n",
    "\n",
    "N_DOCS = len(DOCS)\n",
    "print(f\"Loaded {N_DOCS} documents.\")\n",
    "print(\"Example mapping:\")\n",
    "for i in range(min(3, N_DOCS)):\n",
    "    print(f\"  docID={i} -> {DOCID_TO_PATH[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1087bfa",
   "metadata": {},
   "source": [
    "## 3) Preprocessing functions (tokenize, normalize, stem/lemmatize)\n",
    "\n",
    "- Lowercase text, remove punctuation (keep alphanumerics), and split into tokens.\n",
    "- Apply stemming (Porter). Optionally lemmatization can be toggled.\n",
    "- Reuse these for both documents and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c8f3d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'runner', 's', 'run']\n"
     ]
    }
   ],
   "source": [
    "# Locate the ZIP file (assumes a single corpus zip in the workspace root)\n",
    "workspace_dir = Path.cwd()\n",
    "zip_candidates = list(workspace_dir.glob('Corpus-*.zip'))\n",
    "if not zip_candidates:\n",
    "    # Also try relative path if launched from subfolder\n",
    "    zip_candidates = list(Path('.').glob('Corpus-*.zip'))\n",
    "\n",
    "if not zip_candidates:\n",
    "    raise FileNotFoundError(\"Corpus ZIP not found. Ensure a file named 'Corpus-*.zip' exists next to this notebook.\")\n",
    "\n",
    "corpus_zip = zip_candidates[0]\n",
    "corpus_dir = workspace_dir / 'corpus'\n",
    "corpus_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Unzipping: {corpus_zip} -> {corpus_dir}\")\n",
    "with zipfile.ZipFile(corpus_zip, 'r') as zf:\n",
    "    zf.extractall(corpus_dir)\n",
    "\n",
    "# Recursively collect .txt files (some zips create a top-level folder)\n",
    "text_files = sorted([p for p in corpus_dir.rglob('*.txt')])\n",
    "if not text_files:\n",
    "    raise RuntimeError(\"No .txt files found after unzipping. Check the archive structure.\")\n",
    "\n",
    "# Load into memory and create ID mappings\n",
    "DOCS.clear(); DOCID_TO_PATH.clear(); PATH_TO_DOCID.clear()\n",
    "doc_id = 0\n",
    "for path in text_files:\n",
    "    try:\n",
    "        text = path.read_text(encoding='utf-8', errors='ignore')\n",
    "    except Exception:\n",
    "        text = path.read_text(errors='ignore')  # fallback\n",
    "    DOCS[doc_id] = text\n",
    "    rel = str(path.relative_to(corpus_dir))\n",
    "    DOCID_TO_PATH[doc_id] = rel\n",
    "    PATH_TO_DOCID[rel] = doc_id\n",
    "    doc_id += 1\n",
    "\n",
    "N_DOCS = len(DOCS)\n",
    "print(f\"Loaded {N_DOCS} documents.\")\n",
    "print(\"Example mapping:\")\n",
    "for i in range(min(3, N_DOCS)):\n",
    "    print(f\"  docID={i} -> {DOCID_TO_PATH[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e133c2",
   "metadata": {},
   "source": [
    "## 4) Build inverted index (dictionary + postings)\n",
    "\n",
    "- For each document: preprocess, count term frequencies.\n",
    "- Dictionary maps term -> df and postings list of (docID, raw_tf).\n",
    "- Progress prints are shown for batches of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b174056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building inverted index...\n",
      "  processed 41/41 docs\n",
      "Index built: |V|=4250, postings terms=4250\n"
     ]
    }
   ],
   "source": [
    "def build_index(use_lemma: bool = False):\n",
    "    global VOCAB_DF, POSTINGS\n",
    "    VOCAB_DF = {}\n",
    "    postings_dd = defaultdict(list)\n",
    "\n",
    "    print(\"Building inverted index...\")\n",
    "    for i in range(N_DOCS):\n",
    "        tokens = preprocess(DOCS[i], use_lemma=use_lemma)\n",
    "        tf = Counter(tokens)\n",
    "        for term, freq in tf.items():\n",
    "            postings_dd[term].append((i, freq))\n",
    "        if (i + 1) % 50 == 0 or i == N_DOCS - 1:\n",
    "            print(f\"  processed {i+1}/{N_DOCS} docs\")\n",
    "\n",
    "    # finalize structures\n",
    "    POSTINGS.clear()\n",
    "    for term, plist in postings_dd.items():\n",
    "        # sort postings by docID to keep order stable\n",
    "        plist.sort(key=lambda x: x[0])\n",
    "        POSTINGS[term] = plist\n",
    "        VOCAB_DF[term] = len(plist)\n",
    "\n",
    "    print(f\"Index built: |V|={len(VOCAB_DF)}, postings terms={len(POSTINGS)}\")\n",
    "\n",
    "# Build index now\n",
    "build_index(use_lemma=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a2492",
   "metadata": {},
   "source": [
    "## 5) Document vector normalization (lnc)\n",
    "\n",
    "- For each document term with raw_tf > 0: weight = 1 + log10(raw_tf)\n",
    "- Compute L2 norm per document and store in `DOC_LN_FACTORS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48e2c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed document length normalization factors.\n"
     ]
    }
   ],
   "source": [
    "def compute_doc_lengths():\n",
    "    global DOC_LN_FACTORS\n",
    "    DOC_LN_FACTORS = {i: 0.0 for i in range(N_DOCS)}\n",
    "\n",
    "    # accumulate squared weights per doc\n",
    "    accum = defaultdict(float)\n",
    "    for term, plist in POSTINGS.items():\n",
    "        for doc_id, raw_tf in plist:\n",
    "            if raw_tf > 0:\n",
    "                w = 1.0 + math.log10(raw_tf)\n",
    "                accum[doc_id] += w * w\n",
    "\n",
    "    for doc_id in range(N_DOCS):\n",
    "        DOC_LN_FACTORS[doc_id] = math.sqrt(accum.get(doc_id, 0.0)) or 1.0  # avoid div by zero\n",
    "\n",
    "    print(\"Computed document length normalization factors.\")\n",
    "\n",
    "compute_doc_lengths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667031e",
   "metadata": {},
   "source": [
    "## 6) Query processing + Soundex fallback\n",
    "\n",
    "- Preprocess queries the same way as documents.\n",
    "- If a term is not in the dictionary, attempt to find Soundex code matches from existing dictionary terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00b88e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soundex utilities ready.\n"
     ]
    }
   ],
   "source": [
    "# Soundex implementation (American Soundex)\n",
    "_soundex_map = {\n",
    "    'b': '1', 'f': '1', 'p': '1', 'v': '1',\n",
    "    'c': '2', 'g': '2', 'j': '2', 'k': '2', 'q': '2', 's': '2', 'x': '2', 'z': '2',\n",
    "    'd': '3', 't': '3',\n",
    "    'l': '4',\n",
    "    'm': '5', 'n': '5',\n",
    "    'r': '6'\n",
    "}\n",
    "\n",
    "def soundex(word: str) -> str:\n",
    "    if not word:\n",
    "        return \"0000\"\n",
    "    word = re.sub(r\"[^a-zA-Z]\", \"\", word).lower()\n",
    "    if not word:\n",
    "        return \"0000\"\n",
    "    first = word[0].upper()\n",
    "    digits = []\n",
    "    prev = _soundex_map.get(word[0], '')\n",
    "    for ch in word[1:]:\n",
    "        # Vowels and the letters H, W, Y act as separators\n",
    "        if ch in 'aeiouyhw':\n",
    "            code = ''\n",
    "        else:\n",
    "            code = _soundex_map.get(ch, '')\n",
    "        if code != '' and code != prev:\n",
    "            digits.append(code)\n",
    "        prev = code\n",
    "    code = first + ''.join(digits)\n",
    "    code = (code + '000')[:4]\n",
    "    return code\n",
    "\n",
    "# Build a code->terms mapping for dictionary words (on demand)\n",
    "_sdx_to_terms = None\n",
    "\n",
    "def build_soundex_index():\n",
    "    global _sdx_to_terms\n",
    "    _sdx_to_terms = defaultdict(set)\n",
    "    for term in VOCAB_DF.keys():\n",
    "        _sdx_to_terms[soundex(term)].add(term)\n",
    "\n",
    "\n",
    "def preprocess_query(text: str) -> List[str]:\n",
    "    return preprocess(text, use_lemma=False)\n",
    "\n",
    "\n",
    "def expand_oov_terms_with_soundex(q_terms: List[str]) -> List[str]:\n",
    "    # For terms not present, try to find a single best soundex match by highest df (then lexicographic)\n",
    "    global _sdx_to_terms\n",
    "    if _sdx_to_terms is None:\n",
    "        build_soundex_index()\n",
    "\n",
    "    expanded = []\n",
    "    replacements = {}\n",
    "    for t in q_terms:\n",
    "        if t in VOCAB_DF:\n",
    "            expanded.append(t)\n",
    "        else:\n",
    "            sdx = soundex(t)\n",
    "            cands = list(_sdx_to_terms.get(sdx, []))\n",
    "            if cands:\n",
    "                best = sorted(cands, key=lambda c: (-VOCAB_DF.get(c, 0), c))[0]\n",
    "                expanded.append(best)\n",
    "                replacements[t] = best\n",
    "            else:\n",
    "                expanded.append(t)  # keep as-is; no matches\n",
    "    if replacements:\n",
    "        print(\"Soundex replacements:\", \", \".join(f\"{k}->{v}\" for k, v in replacements.items()))\n",
    "    return expanded\n",
    "\n",
    "print(\"Soundex utilities ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb191ce5",
   "metadata": {},
   "source": [
    "## 7) Similarity computation and ranked retrieval (lnc.ltc)\n",
    "\n",
    "- Query weights: ltc (log tf + idf with idf = log10(N/df) + cosine normalization)\n",
    "- Document weights: lnc (log tf + cosine normalization, no idf)\n",
    "- Cosine similarity using postings lists for efficiency.\n",
    "- Return top-10 sorted by score desc, then docID asc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0844f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking function ready.\n"
     ]
    }
   ],
   "source": [
    "def rank_query(query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    # Preprocess + Soundex expansion for OOV\n",
    "    q_tokens = preprocess_query(query)\n",
    "    q_terms = expand_oov_terms_with_soundex(q_tokens)\n",
    "\n",
    "    # Query term frequencies and ltc weights\n",
    "    q_tf = Counter(q_terms)\n",
    "    q_weights = {}\n",
    "    for term, raw_tf in q_tf.items():\n",
    "        if raw_tf <= 0:\n",
    "            continue\n",
    "        tfw = 1.0 + math.log10(raw_tf)\n",
    "        df = VOCAB_DF.get(term)\n",
    "        if not df:\n",
    "            # for terms not in vocab (e.g., no soundex match resolved), skip from vector\n",
    "            continue\n",
    "        idf = math.log10(N_DOCS / df) if df > 0 else 0.0\n",
    "        q_weights[term] = tfw * idf\n",
    "\n",
    "    # Normalize query vector\n",
    "    q_norm = math.sqrt(sum(w * w for w in q_weights.values())) or 1.0\n",
    "    for t in list(q_weights.keys()):\n",
    "        q_weights[t] /= q_norm\n",
    "\n",
    "    # Accumulate cosine scores using postings\n",
    "    scores = defaultdict(float)\n",
    "    for term, qw in q_weights.items():\n",
    "        plist = POSTINGS.get(term, [])\n",
    "        for doc_id, raw_tf in plist:\n",
    "            if raw_tf > 0:\n",
    "                dw_tf = 1.0 + math.log10(raw_tf)  # lnc weight numerator\n",
    "                denom = DOC_LN_FACTORS.get(doc_id, 1.0)\n",
    "                if denom == 0.0:\n",
    "                    denom = 1.0\n",
    "                dw = dw_tf / denom\n",
    "                scores[doc_id] += qw * dw\n",
    "\n",
    "    # Prepare top-k results sorted by score desc, then docID asc\n",
    "    results = sorted(scores.items(), key=lambda x: (-x[1], x[0]))[:top_k]\n",
    "    return results\n",
    "\n",
    "print(\"Ranking function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1fa94",
   "metadata": {},
   "source": [
    "## 8) Run test cases and display outputs\n",
    "\n",
    "- Use example queries from the provided PDF (paste a couple here).\n",
    "- Show top-10 results with similarity scores and filenames.\n",
    "- Tie-breaking: score desc, then docID asc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45cd560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q1: Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
      "Top-10 results:\n",
      " 1. docID=  40  score=0.199932  file=Corpus\\zomato.txt\n",
      " 2. docID=  33  score=0.117688  file=Corpus\\swiggy.txt\n",
      " 3. docID=  13  score=0.068714  file=Corpus\\instagram.txt\n",
      " 4. docID=  16  score=0.064887  file=Corpus\\messenger.txt\n",
      " 5. docID=  39  score=0.061525  file=Corpus\\youtube.txt\n",
      " 6. docID=   8  score=0.057806  file=Corpus\\Discord.txt\n",
      " 7. docID=   4  score=0.053515  file=Corpus\\bing.txt\n",
      " 8. docID=  25  score=0.050172  file=Corpus\\reddit.txt\n",
      " 9. docID=  29  score=0.045595  file=Corpus\\skype.txt\n",
      "10. docID=  10  score=0.041589  file=Corpus\\google.txt\n",
      "\n",
      "================================================================================\n",
      "Q2: Warwickshire, came from an ancient family and was the heiress to some land\n",
      "Top-10 results:\n",
      " 1. docID=  28  score=0.108578  file=Corpus\\shakespeare.txt\n",
      " 2. docID=  15  score=0.027817  file=Corpus\\levis.txt\n",
      " 3. docID=  10  score=0.022757  file=Corpus\\google.txt\n",
      " 4. docID=   0  score=0.022277  file=Corpus\\Adobe.txt\n",
      " 5. docID=  19  score=0.021202  file=Corpus\\nike.txt\n",
      " 6. docID=  40  score=0.018587  file=Corpus\\zomato.txt\n",
      " 7. docID=  12  score=0.017203  file=Corpus\\huawei.txt\n",
      " 8. docID=   5  score=0.015607  file=Corpus\\blackberry.txt\n",
      " 9. docID=  26  score=0.014107  file=Corpus\\reliance.txt\n",
      "10. docID=  29  score=0.013385  file=Corpus\\skype.txt\n"
     ]
    }
   ],
   "source": [
    "def print_results(results: List[Tuple[int, float]], header: str = None):\n",
    "    if header:\n",
    "        print(header)\n",
    "    for rank, (doc_id, score) in enumerate(results, 1):\n",
    "        print(f\"{rank:2d}. docID={doc_id:4d}  score={score:.6f}  file={DOCID_TO_PATH.get(doc_id, '?')}\")\n",
    "\n",
    "# Example test queries \n",
    "queries = [\n",
    "    \"Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\", \"Warwickshire, came from an ancient family and was the heiress to some land\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    res = rank_query(q, top_k=10)\n",
    "    print_results(res, header=\"Top-10 results:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dab18d",
   "metadata": {},
   "source": [
    "## 9) Wrap up and optionally save results\n",
    "\n",
    "- Save last query results to a CSV file in the workspace for your records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad6626d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top-10 results of last query to: E:\\COLLEGE MATERIAL\\SEMESTER 7\\Information Retrieval\\Assignment_Vector_Space_Model\\vsm_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results of the last executed query loop (if any) to CSV\n",
    "try:\n",
    "    import csv\n",
    "    last_q = queries[-1]\n",
    "    last_results = rank_query(last_q, top_k=10)\n",
    "    out_path = Path('vsm_results.csv')\n",
    "    with out_path.open('w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['rank', 'docID', 'score', 'filename'])\n",
    "        for rank, (doc_id, score) in enumerate(last_results, 1):\n",
    "            writer.writerow([rank, doc_id, f\"{score:.6f}\", DOCID_TO_PATH.get(doc_id, '?')])\n",
    "    print(f\"Saved top-10 results of last query to: {out_path.resolve()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save results: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
